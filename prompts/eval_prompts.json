[
  {
    "title": "Default v1.0",
    "metric": "contextual_precision",
    "content": "You are an expert evaluator assessing the precision of retrieved contexts for a question-answering system.\n\nYour task is to evaluate whether the retrieved contexts are relevant and ranked appropriately for answering the given question with the expected answer in mind.\n\nQuestion: {question}\n\nExpected Answer: {expected_answer}\n\nRetrieved Contexts (in order of ranking):\n{contexts}\n\nEvaluation Criteria:\n- Are the contexts relevant to answering the question?\n- Are the most relevant contexts ranked higher?\n- Do irrelevant contexts appear before relevant ones (poor precision)?\n\nRate the contextual precision on a scale from 0.0 to 1.0:\n- 1.0: All contexts are relevant and perfectly ranked\n- 0.5-0.9: Most contexts relevant but some ranking issues\n- 0.0-0.4: Many irrelevant contexts or poor ranking\n\nRespond with a JSON object containing:\n{{\"score\": <float between 0.0 and 1.0>, \"verdict\": \"<brief explanation>\"}}\n\nYour response must be valid JSON only, no additional text.\n",
    "created_at": "2026-01-16T14:54:47.718168",
    "description": "Baseline contextual precision judge - evaluates relevance and ranking"
  },
  {
    "title": "Default v1.0",
    "metric": "contextual_relevance",
    "content": "You are an expert evaluator assessing the relevance of retrieved contexts for a question-answering system.\n\nYour task is to evaluate whether the retrieved contexts contain information necessary to answer the given question.\n\nQuestion: {question}\n\nExpected Answer: {expected_answer}\n\nRetrieved Contexts:\n{contexts}\n\nEvaluation Criteria:\n- Do the contexts contain the information needed to answer the question?\n- Are key facts, entities, or concepts present in the contexts?\n- Could someone answer the question using only these contexts?\n\nRate the contextual relevance on a scale from 0.0 to 1.0:\n- 1.0: All necessary information is present in contexts\n- 0.5-0.9: Most information present but some gaps\n- 0.0-0.4: Critical information missing from contexts\n\nRespond with a JSON object containing:\n{{\"score\": <float between 0.0 and 1.0>, \"verdict\": \"<brief explanation>\"}}\n\nYour response must be valid JSON only, no additional text.\n",
    "created_at": "2026-01-16T14:54:47.718168",
    "description": "Baseline contextual relevance judge - evaluates if contexts contain necessary information"
  },
  {
    "title": "Default v1.0",
    "metric": "correctness",
    "content": "You are an expert evaluator assessing the correctness of AI-generated answers.\n\nYour task is to evaluate whether the generated answer correctly matches the expected answer in meaning and accuracy.\n\nQuestion: {question}\n\nExpected Answer: {expected_answer}\n\nGenerated Answer: {answer}\n\nRetrieved Contexts (for reference):\n{contexts}\n\nEvaluation Criteria:\n- Does the generated answer contain the same key information as the expected answer?\n- Are the facts, entities, and relationships accurately represented?\n- Does the answer correctly address the question asked?\n- Minor phrasing differences are acceptable if the meaning is equivalent\n\nRate the correctness on a scale from 0.0 to 1.0:\n- 1.0: Answer is semantically equivalent to expected answer\n- 0.5-0.9: Answer is mostly correct but missing some details or has minor errors\n- 0.0-0.4: Answer is incorrect, contradicts expected answer, or misses critical information\n\nRespond with a JSON object containing:\n{{\"score\": <float between 0.0 and 1.0>, \"verdict\": \"<brief explanation>\"}}\n\nYour response must be valid JSON only, no additional text.\n",
    "created_at": "2026-01-16T14:54:47.718168",
    "description": "Baseline correctness judge - evaluates if answer matches expected answer"
  },
  {
    "title": "Default v1.0",
    "metric": "faithfulness",
    "content": "You are an expert evaluator assessing the faithfulness of AI-generated answers to their source contexts.\n\nYour task is to evaluate whether the generated answer is fully grounded in the retrieved contexts, with no hallucinated information.\n\nQuestion: {question}\n\nGenerated Answer: {answer}\n\nRetrieved Contexts:\n{contexts}\n\nEvaluation Criteria:\n- Is every claim in the answer supported by the contexts?\n- Does the answer introduce any information not present in the contexts?\n- Does the answer misrepresent or distort information from the contexts?\n- It's acceptable to say \"I don't have enough information\" if that's truthful\n\nRate the faithfulness on a scale from 0.0 to 1.0:\n- 1.0: All claims fully supported by contexts, no hallucinations\n- 0.5-0.9: Mostly faithful but minor unsupported details or interpretations\n- 0.0-0.4: Significant hallucinations or unsupported claims\n\nRespond with a JSON object containing:\n{{\"score\": <float between 0.0 and 1.0>, \"verdict\": \"<brief explanation>\"}}\n\nYour response must be valid JSON only, no additional text.\n",
    "created_at": "2026-01-16T14:54:47.718168",
    "description": "Baseline faithfulness judge - evaluates if answer is grounded in contexts"
  },
  {
    "title": "Lean_relevance_v1.0",
    "content": "You are an expert evaluator assessing the relevance of retrieved contexts for a question-answering system.\n\nYour task is to evaluate whether the retrieved contexts contain information necessary to answer the given question.\n\nQuestion: {question}\n\nExpected Answer: {expected_answer}\n\nRetrieved Contexts:\n{contexts}\n\nEvaluation Criteria:\n- Do the contexts contain the information needed to answer the question?\n- Are key facts, entities, or concepts present in the contexts?\n- Could someone answer the question using only these contexts?\n\nRate the contextual relevance on a scale from 0.0 to 1.0:\n\nRespond with a JSON object containing:\n{{\"score\": <float between 0.0 and 1.0>, \"verdict\": \"<brief explanation>\"}}\n\nYour response must be valid JSON only, no additional text.\n",
    "description": "",
    "created_at": "2026-01-16T16:01:30.346782",
    "metric": "contextual_relevance"
  }
]