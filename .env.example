# ==============================================================================
# RAG Evaluation Framework Configuration
# ==============================================================================
# Copy this file to .env and update with your values
OPENAI_API_KEY=your-api-key-here

EMBEDDING_MODEL=text-embedding-3-small
LLM_MODEL=gpt-4o-mini

VECTOR_STORE_DIR=./storage/chroma
COLLECTION_NAME=knowledge_base
RETRIEVAL_STRATEGY=vector
TOP_K=3
CHUNK_SIZE=512
CHUNK_OVERLAP=50
JUDGE_MODEL=gpt-4o-mini
JUDGE_NUM_SAMPLES=1
JUDGE_TEMPERATURE=0.0
INCLUDE_METRIC_REASONS=false
MAX_CONTEXTS_FOR_EVAL=2
OVERALL_SCORE_WEIGHTS={}
RAG_SYSTEM_PROMPT_TITLE=Default v1.0
EVAL_PROMPT_CONTEXTUAL_PRECISION=Default v1.0
EVAL_PROMPT_CONTEXTUAL_RELEVANCE=Default v1.0
EVAL_PROMPT_CORRECTNESS=Default v1.0
EVAL_PROMPT_FAITHFULNESS=Default v1.0
API_BASE_URL=http://127.0.0.1:8000
INPUT_TOKEN_PRICE_PER_MILLION=0.15
OUTPUT_TOKEN_PRICE_PER_MILLION=0.6